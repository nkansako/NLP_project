from collections import OrderedDict

import matplotlib.pyplot as plt
from nltk import FreqDist

import nlp_config as config
import nlp_helpers as helpers
import preprocessing
import fuzzy
import numpy as np
import csv
import string

# constants
results_path = 'results/8b/'


# TASK DESCRIPTION
# 8. Compare the phonetic of starting word and ending word of each line of the poem.
# Use the “Fuzzy” library in python to generate the character string that identifies phonetically similar words,
# and then use edit distance to compute the phonetic similarity between two words.
# Leave the size of the generated string open.
# The distance between the phonetic generated vectors corresponding to starting/ending word is generated like this:
# Assume L1 is the first word phonetic string, and L2 is the second word phonetic string generated by Fuzzy library.
# Then the phonetic association Sim(L1,L2)= 2*S/ (length(L1)+length(L2)) where S is the length of the largest substring,
# which is common to both L1 and L2.
# Calculate the value of Sim(L1,L2) for each line of the poem and size the result in excel file.
# Find out whether some curve fitting (polynomial, exponential or zipf can be fitted to the data).
# Motivate your answer and display appropriate plotting.


def rhyme_analysis(chapters: list, analysis_type: helpers.AnalysisTypes, threshold: float, book_ref: str):

    rhyme_types = []
    all_lengths = []

    for chapter in chapters:
        lengths = []

        if len(chapter) == 1:
            text = ''
            try:
                text = helpers.prepare_string(chapter[0], analysis_type)
                soundex = fuzzy.Soundex(len(text))
                length = soundex(text)
                all_lengths.append(length)
            except UnicodeEncodeError:
                print("Add exception character to nlp_config.trim_values: " + text)
            continue  # we skip chapters with 1 lines

        for line in chapter:
            # print(line)
            text = helpers.prepare_string(line, analysis_type)
            # print(text)
            # exit()
            lines_per_batch = []

            soundex = fuzzy.Soundex(len(text))
            try:
                length = soundex(text)
                lengths.append(length)
                all_lengths.append(length)
                lines_per_batch.append(text)
            except UnicodeEncodeError:
                print("Add exception character to nlp_config.trim_values: " + text)
                # exit()

        results_length = len(lengths)
        matches = np.zeros([results_length, results_length])
        for j in range(results_length):
            for k in range(results_length):
                if j == k:
                    matches[j][k] = 1
                else:
                    S = len(largest_common_substring(lengths[j], lengths[k], len(lengths[j]), len(lengths[k])))
                    try:
                        similarity = 2 * S / (len(lengths[j]) + len(lengths[k]))
                    except ZeroDivisionError:
                        similarity = 0
                    matches[j][k] = similarity

        # print(matches)
        # exit()
        unique_pattern_letters = string.ascii_uppercase
        current_pattern_letter_index = 0

        current_pattern = [""] * results_length
        current_pattern[0] = unique_pattern_letters[current_pattern_letter_index]
        next_letter = 1
        for j in range(results_length):
            br = False
            for k in range(results_length):
                if j != k:
                    if current_pattern[k] == "":
                        # print("checking for j:" + str(j))
                        if matches[j][k] >= threshold:
                            # print(unique_pattern_letters[current_pattern_letter_index])
                            # print('changing')
                            current_pattern[k] = unique_pattern_letters[current_pattern_letter_index]
                            next_letter += 1
            if next_letter > 1:  # use a letter at least twice
                current_pattern_letter_index += 1
                next_letter = 0
                if current_pattern_letter_index >= len(unique_pattern_letters):
                    br = True
                    break
            if br:
                break

        for i in range(len(current_pattern)):
            if current_pattern[i] == "":
                current_pattern_letter_index += 1 * next_letter
                current_pattern[i] = unique_pattern_letters[current_pattern_letter_index]
                next_letter = 1

        # print(current_pattern)

        rhyme_types.append("".join(current_pattern))
        # if "".join(current_pattern) == pattern:
        #     final_matches_found += 1

        # i += results_length
    # print(pattern + ": final_matches_found=" + str(final_matches_found))
    # print("len(book)" + str(len(book)))

    # save all lengths into CSV
    save_to_CSV(all_lengths, book_ref)

    return FreqDist(rhyme_types)


def save_to_CSV(data, book_ref):
    path = results_path + book_ref + "_phonetic_analysis"
    with open(path + '.csv', mode='w') as csv_file:
        csv_writer = csv.writer(csv_file, delimiter=',', quotechar='"', quoting=csv.QUOTE_MINIMAL)
        for item in data:
            csv_writer.writerow([item])
    return


def largest_common_substring(X, Y, m, n):
    LCSuff = [[0 for i in range(n + 1)]
              for j in range(m + 1)]

    # To store length of the
    # longest common substring
    length = 0

    # To store the index of the cell
    # which contains the maximum value.
    # This cell's index helps in building
    # up the longest common substring
    # from right to left.
    row, col = 0, 0

    # Following steps build LCSuff[m+1][n+1]
    # in bottom up fashion.
    for i in range(m + 1):
        for j in range(n + 1):
            if i == 0 or j == 0:
                LCSuff[i][j] = 0
            elif X[i - 1] == Y[j - 1]:
                LCSuff[i][j] = LCSuff[i - 1][j - 1] + 1
                if length < LCSuff[i][j]:
                    length = LCSuff[i][j]
                    row = i
                    col = j
            else:
                LCSuff[i][j] = 0

    # if true, then no common substring exists
    if length == 0:
        # print("No Common Substring")
        return ""

    # allocate space for the longest
    # common substring
    resultStr = ['0'] * length

    # traverse up diagonally form the
    # (row, col) cell until LCSuff[row][col] != 0
    while LCSuff[row][col] != 0:
        length -= 1
        resultStr[length] = X[row - 1]  # or Y[col-1]

        # move diagonally up to previous cell
        row -= 1
        col -= 1

    # required longest common substring
    # print(''.join(resultStr))

    return ''.join(resultStr)


def task8():
    raw_books = helpers.getBooks()

    for book_ref, raw_book in raw_books.items():
        title_ = config.books[book_ref]['title']
        chapters = preprocessing.preprocess(raw_book)
        first_and_last_words = preprocessing.only_first_and_last_words(chapters)
        for chapter_name, chapter_data  in first_and_last_words.items():
            print(chapter_name)
        # print(first_and_last_words)
        # chapters = preprocessing.book_to_verses(raw_book)
        #
        # for analysis_type in helpers.AnalysisTypes:
        #     # print("Analyzing for:" + analysis_type.value)
        #     threshold = config.similarity_threshold
        #     fd = rhyme_analysis(chapters, analysis_type, threshold, book_ref)
        #     title = "Phonetic analysis for *" + analysis_type.value + "*, book: " + title_ +\
        #             ", similarity threshold: " + str(threshold)
        #     plt.figure(figsize=(30, 20))
        #     fd.plot(config.display_top, cumulative=False, title=title)


if __name__ == "__main__":
    task8()
